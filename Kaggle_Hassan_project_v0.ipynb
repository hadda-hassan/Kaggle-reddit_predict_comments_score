{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_Hassan_project_v0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6/9tDuAop4EBUfM/EonRt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadda-hassan/Kaggle-reddit_predict_comments_score/blob/main/Kaggle_Hassan_project_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjrskAAIW21T"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqeivXA1XEYn"
      },
      "source": [
        "# fonction interact with modeles\n",
        "def model_diagnostics(model, pr=True):\n",
        "    \"\"\"\n",
        "    Returns and prints the R-squared, RMSE and the MAE for a trained model\n",
        "    \"\"\"\n",
        "    y_predicted = model.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_predicted)\n",
        "    mse = mean_squared_error(y_test, y_predicted)\n",
        "    mae = mean_absolute_error(y_test, y_predicted)\n",
        "    if pr:\n",
        "        print(f\"R-Sq: {r2:.4}\")\n",
        "        print(f\"RMSE: {np.sqrt(mse)}\")\n",
        "        print(f\"MAE: {mae}\")\n",
        "    \n",
        "    return [r2,np.sqrt(mse),mae]\n",
        "def plot_residuals(y_test, y_predicted):\n",
        "    \"\"\"\"\n",
        "    Plots the distribution for actual and predicted values of the target variable. Also plots the distribution for the residuals\n",
        "    \"\"\"\n",
        "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
        "    sns.distplot(y_test, ax=ax0, kde = False)\n",
        "    ax0.set(xlabel='Test scores')\n",
        "    sns.distplot(y_predicted, ax=ax1, kde = False)\n",
        "    ax1.set(xlabel=\"Predicted scores\")\n",
        "    plt.show()\n",
        "    fig, ax2 = plt.subplots()\n",
        "    sns.distplot((y_test-y_predicted), ax = ax2,kde = False)\n",
        "    ax2.set(xlabel=\"Residuals\")\n",
        "    plt.show()\n",
        "def y_test_vs_y_predicted(y_test,y_predicted):\n",
        "    \"\"\"\n",
        "    Produces a scatter plot for the actual and predicted values of the target variable\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.scatter(y_test, y_predicted)\n",
        "    ax.set_xlabel(\"Test Scores\")\n",
        "    ax.set_ylim([-75, 1400])\n",
        "    ax.set_ylabel(\"Predicted Scores\")\n",
        "    plt.show()\n",
        "def get_feature_importance(model):\n",
        "    \"\"\"\n",
        "    For fitted tree based models, get_feature_importance can be used to get the feature importance as a tidy output\n",
        "    \"\"\"\n",
        "    X_non_text = pd.get_dummies(df[cat_cols])\n",
        "    features = numeric_cols + bool_cols + list(X_non_text.columns)\n",
        "    feature_importance = dict(zip(features, model.feature_importances_))\n",
        "    for name, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"{name:<30}: {importance:>6.2%}\")\n",
        "        print(f\"\\nTotal importance: {sum(feature_importance.values()):.2%}\")\n",
        "    return feature_importance"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}